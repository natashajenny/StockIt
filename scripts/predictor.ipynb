{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "import pywt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Reshape\n",
    "from model import Company, StockLog, IndexLog, start_engine\n",
    "from sqlalchemy import asc\n",
    "from datetime import date, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from workalendar.oceania import NewSouthWales\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = start_engine()\n",
    "start = datetime.strptime('2009-07-01', '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypers\n",
    "BATCH_SIZE = 20\n",
    "EPOCHS = 20\n",
    "DROPOUT = 0.25\n",
    "LSTM_UNITS = 100\n",
    "TRAIN_LIMIT = 1.0\n",
    "DAYS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = ['WOW', 'ABC', 'SYD', 'IAG', 'AGL']\n",
    "companies = [c for c in Company().query().all() if c.code not in diff]\n",
    "# companies = [Company().query().get('ABC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AWC\n",
      "Training model\n",
      "Writing to database\n",
      "Processing ALU\n",
      "Training model\n",
      "Writing to database\n",
      "Processing NAB\n",
      "Training model\n",
      "Writing to database\n",
      "Processing AMC\n",
      "Training model\n",
      "Writing to database\n",
      "Processing AMP\n",
      "Training model\n",
      "Writing to database\n",
      "Processing ANN\n",
      "Training model\n",
      "Writing to database\n",
      "Processing ANZ\n",
      "Training model\n",
      "Writing to database\n",
      "Processing APA\n",
      "Training model\n",
      "Writing to database\n",
      "Processing ALL\n",
      "Training model\n",
      "Writing to database\n",
      "Processing ASX\n",
      "Training model\n",
      "Writing to database\n",
      "Processing AST\n",
      "Training model\n",
      "Writing to database\n",
      "Processing BOQ\n",
      "Training model\n",
      "Writing to database\n",
      "Processing BEN\n",
      "Training model\n",
      "Writing to database\n",
      "Processing BHP\n",
      "Training model\n",
      "Writing to database\n",
      "Processing BSL\n",
      "Training model\n",
      "Writing to database\n",
      "Processing BLD\n",
      "Training model\n",
      "Writing to database\n",
      "Processing BXB\n",
      "Training model\n",
      "Writing to database\n",
      "Processing CTX\n",
      "Training model\n",
      "Writing to database\n",
      "Processing CGF\n",
      "Training model\n",
      "Writing to database\n",
      "Processing CHC\n",
      "Training model\n",
      "Writing to database\n",
      "Processing CIM\n",
      "Training model\n",
      "Writing to database\n",
      "Processing CWY\n",
      "Training model\n",
      "Writing to database\n",
      "Processing CCL\n",
      "Training model\n",
      "Writing to database\n",
      "Processing COH\n",
      "Training model\n",
      "Writing to database\n",
      "Processing CBA\n",
      "Training model\n",
      "Writing to database\n",
      "Processing CPU\n",
      "Training model\n",
      "Writing to database\n",
      "Processing CWN\n",
      "Training model\n",
      "Writing to database\n",
      "Processing CSL\n",
      "Training model\n",
      "Writing to database\n",
      "Processing DXS\n",
      "Training model\n",
      "Writing to database\n",
      "Processing DMP\n",
      "Training model\n",
      "Writing to database\n",
      "Processing DOW\n",
      "Training model\n",
      "Writing to database\n",
      "Processing EVN\n",
      "Training model\n",
      "Writing to database\n",
      "Processing FLT\n",
      "Training model\n",
      "Writing to database\n",
      "Processing FMG\n",
      "Training model\n",
      "Writing to database\n",
      "Processing GMG\n",
      "Training model\n",
      "Writing to database\n",
      "Processing GPT\n",
      "Training model\n",
      "Writing to database\n",
      "Processing ILU\n",
      "Training model\n",
      "Writing to database\n",
      "Processing IPL\n",
      "Training model\n",
      "Writing to database\n",
      "Processing JHX\n",
      "Training model\n",
      "Writing to database\n",
      "Processing JHG\n",
      "Training model\n",
      "Writing to database\n",
      "Processing JBH\n",
      "Training model\n",
      "Writing to database\n",
      "Processing MQG\n",
      "Training model\n",
      "Writing to database\n",
      "Processing MFG\n",
      "Training model\n",
      "Writing to database\n",
      "Processing MGR\n",
      "Training model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-07032fe28681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Batch up test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for company in companies:\n",
    "\n",
    "    print('Processing', company.code)\n",
    "\n",
    "    stock_log = StockLog().query().filter(StockLog.company==company)\n",
    "    df = pd.read_sql(stock_log.statement, engine)\n",
    "    df.sort_values(by='date', inplace=True)\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    sectors = {'Consumer Discretionary': 'discretionary',\n",
    "               'Consumer Staples': 'staples',\n",
    "               'Energy': 'energy',\n",
    "               'Financials': 'financials',\n",
    "               'Health Care': 'healthcare',\n",
    "               'Industrials': 'industrials',\n",
    "               'Information Technology': 'infotech',\n",
    "               'Materials': 'materials',\n",
    "               'Telecommunication Services': 'telecom',\n",
    "               'Utilities': 'utilities',\n",
    "               'Real Estate': 'realestate'}\n",
    "    sectors_cpy = copy.deepcopy(sectors)\n",
    "    sectors_cpy.pop(company.sector)\n",
    "    sectors_cpy = set([sectors_cpy[sector] for sector in sectors_cpy])\n",
    "\n",
    "    index_log = IndexLog().query()\n",
    "    fi = pd.read_sql(index_log.statement, engine)\n",
    "    indicies = set(fi['index'])\n",
    "    indicies -= sectors_cpy\n",
    "\n",
    "    for idx in indicies:\n",
    "        new = fi.loc[fi['index'] == idx]\n",
    "        new = new.rename(columns={'value': idx})\n",
    "        new.sort_values(by='date', inplace=True)\n",
    "        new.set_index('date', inplace=True)\n",
    "        new.drop(new.columns[[0]], axis=1, inplace=True)\n",
    "        df = df.merge(new, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "    df.rename(columns={sectors[company.sector]: 'sector'}, inplace=True)    \n",
    "    df.drop(df[df.opening == 0].index, inplace=True)\n",
    "    df.dropna(subset=['opening'], inplace=True)\n",
    "    \n",
    "    df['prime'].interpolate(method='pad', inplace=True)\n",
    "    df['world'].interpolate(method='pad', inplace=True)\n",
    "    df['pacific'].interpolate(method='pad', inplace=True)\n",
    "    df['axvi'].interpolate(method='pad', inplace=True)\n",
    "    df['sector'].interpolate(method='pad', inplace=True)\n",
    "    df['aud_usd'].interpolate(method='pad', inplace=True)\n",
    "    df['twi'].interpolate(method='pad', inplace=True)\n",
    "    \n",
    "    df.drop(df[df.index < start].index, inplace=True)\n",
    "    \n",
    "    x = np.array(df['closing'])\n",
    "    cA4, cD4, cD3, cD2, cD1 = pywt.wavedec(x, 'haar', level=4)\n",
    "    cA4 = pywt.threshold(cA4, np.std(cA4), mode=\"hard\")\n",
    "    cD4 = pywt.threshold(cD4, np.std(cD4), mode=\"hard\") \n",
    "    cD3 = pywt.threshold(cD3, np.std(cD3), mode=\"hard\") \n",
    "    cD2 = pywt.threshold(cD2, np.std(cD2), mode=\"hard\") \n",
    "    cD1 = pywt.threshold(cD1, np.std(cD1), mode=\"hard\") \n",
    "    tx = pywt.waverec((cA4, cD4, cD3, cD2, cD1), 'haar')\n",
    "    if len(tx) > df.shape[0]:\n",
    "        tx = tx[1:]\n",
    "    df['dwt'] = tx\n",
    "    \n",
    "    df.drop(df.columns[[0, 1, 2, 3, 4, 5, 6, -13, -12, -11, -10]], axis=1, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    dataset = df.values\n",
    "    \n",
    "    # Data split\n",
    "    cut_off = dataset.shape[0] - DAYS\n",
    "    training = dataset[:cut_off, :]\n",
    "    testing = dataset[cut_off - BATCH_SIZE:, :]\n",
    "\n",
    "    # Scale Training\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(training)\n",
    "    test_scaled = scaler.transform(testing)\n",
    "    \n",
    "    # Batch up data\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(BATCH_SIZE, len(training) - DAYS):\n",
    "        x_train.append(train_scaled[i - BATCH_SIZE:i, :-1])\n",
    "        y_train.append(train_scaled[i:i + DAYS, -1])\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    \n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=LSTM_UNITS, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(LSTM(units=LSTM_UNITS))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(DAYS))\n",
    "    \n",
    "    # Run training\n",
    "    print('Training model')\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n",
    "    \n",
    "    # Batch up test data\n",
    "    x_test = []\n",
    "    for i in range(BATCH_SIZE, test_scaled.shape[0]):\n",
    "        x_test.append(test_scaled[i - BATCH_SIZE:i, :-1])\n",
    "    x_test = np.array(x_test)\n",
    "    \n",
    "    # Get prediction\n",
    "    y_test = model.predict(x_test)\n",
    "    x_test = x_test[:, 0, :]\n",
    "    y_cut = y_test[-1:, :]\n",
    "    y_cut = y_cut.transpose()\n",
    "    prediction = np.concatenate((x_test, y_cut), axis=1)\n",
    "    prediction = scaler.inverse_transform(prediction)\n",
    "    prediction = prediction[:, -1]    \n",
    "    \n",
    "    cal = NewSouthWales()\n",
    "    holidays = []\n",
    "    for i in range(DAYS):\n",
    "        holidays.append(cal.add_working_days(max(df.index), i+1))\n",
    "\n",
    "    print('Writing to database')\n",
    "    for i in range(DAYS):\n",
    "        sl = StockLog()\n",
    "        sl.date = holidays[i]\n",
    "        sl.code = company.code\n",
    "        sl.prediction = prediction[i]\n",
    "        sl.save()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
